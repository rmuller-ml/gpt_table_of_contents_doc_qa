{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get questions answered from documentation using GPT API\n",
        "\n",
        "**TLDR**: This notebook shows how to use GPT to answer questions from a big document that has an available index. It is done in 2 steps: (1) for a given question, ask GPT which section/chapter in the index to look for the answer (2) get the content of the section/chapter and include in the question/prompt so the answer can refer to the content (context) to better answer the question.\n",
        "\n",
        "\n",
        "**Motivation**\n",
        "\n",
        "With the availability of the GPT API, we can get natural language answers from data crawled by OpenAI. However, to get answers from private documents, we need to finetune the GPT model on the private documents or include those documents in the context of the question. If the document is too large for the context (maximum 4096 tokens, resulting in max ~3000 words including the answer) we need to be able to first find the sections of the sections of the document that may contain the answer. There are many ways to do this: \n",
        "\n",
        "**Embeddings**\n",
        "\n",
        "\n",
        "1.   Split the document into chunks \n",
        "2.   Embedd this text chunks using GPT\n",
        "3.   Embedd the question and use as context the chunk(s) with maximum cosine distance between chunk and question embedding\n",
        "\n",
        "See https://platform.openai.com/docs/tutorials/web-qa-embeddings\n",
        "\n",
        "**Using the table of contents/index (this notebook)**\n",
        "\n",
        "If the document is structured using chapters, sections and sub-sections, we can ask ChatGPT (for a given question), which section has the higher probability of containing the answer to the question we want answered. Then, we can include the text content of the section into a second question which will help to answer the user's question. \n",
        "\n",
        "Since we cannot force GPT to answer a multiple choice question, we can use the likelihood it assigns for each section a pick the one with maximum likelihood.\n",
        "\n",
        "**Data**\n",
        "\n",
        "As an example we use the OpenAI API documentation. We extract the content and table of contents, then we ask questions.\n",
        "\n",
        "https://platform.openai.com/docs/api-reference\n",
        "\n",
        "To download the html, use a headless browser such as:\n",
        "\n",
        "google-chrome --headless --dump-dom 'https://platform.openai.com/docs/api-reference' > ~/api_reference.html"
      ],
      "metadata": {
        "id": "M0pd2P4oYE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper class where we store the table of contents and content of eash section\n",
        "class TextSection:\n",
        "  def __init__(self, name, text, sub_sections = None):\n",
        "    self.name = name\n",
        "    self.text = text\n",
        "    self.sub_sections = sub_sections\n",
        "  \n",
        "  def get_index_text(self, level=0):\n",
        "    index_text = \" \"*level + self.name\n",
        "    if self.sub_sections:\n",
        "      return  '\\n'.join([index_text]+[s_s.get_index_text(level+1) for s_s in self.sub_sections])\n",
        "    else:\n",
        "      return index_text\n",
        "  \n",
        "  def flatten(self):\n",
        "    sub_section_list = [self]\n",
        "    if self.sub_sections:\n",
        "      for s_s in self.sub_sections:\n",
        "        sub_section_list += s_s.flatten()\n",
        "    return sub_section_list\n"
      ],
      "metadata": {
        "id": "S1Yd38IOr1Xa"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using BeautfulSoup to extract the text from html\n",
        "\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "filename = \"api_reference.html\"\n",
        "html = Path(filename).read_text()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "docs_body = soup.find('div','docs-body')\n",
        "\n",
        "api_reference = TextSection(\"OpenAI API reference\", \"\", [])\n",
        "\n",
        "for i in list(list(docs_body.children)[0].children):\n",
        "  text_section = TextSection(i.h2.get_text(), i.get_text(), [])\n",
        "  if i['class'][1] == 'md':\n",
        "    api_reference.sub_sections.append(text_section)\n",
        "  if i['class'][1] == 'endpoint':\n",
        "    api_reference.sub_sections[-1].sub_sections.append(text_section)   \n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "auM5kSuOttdi"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table of contents (index)\n",
        "print(api_reference.get_index_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Oz0LnK_uOaJ",
        "outputId": "9c292de4-8225-4cc6-b08b-7a39e709dd38"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API reference\n",
            " Introduction\n",
            " Authentication\n",
            " Making requests\n",
            " Models\n",
            "  List models\n",
            "  Retrieve model\n",
            " Completions\n",
            "  Create completion\n",
            " Chat\n",
            "  Create chat completionBeta\n",
            " Edits\n",
            "  Create edit\n",
            " Images\n",
            "  Create imageBeta\n",
            "  Create image editBeta\n",
            "  Create image variationBeta\n",
            " Embeddings\n",
            "  Create embeddings\n",
            " Audio\n",
            "  Create transcriptionBeta\n",
            "  Create translationBeta\n",
            " Files\n",
            "  List files\n",
            "  Upload file\n",
            "  Delete file\n",
            "  Retrieve file\n",
            "  Retrieve file content\n",
            " Fine-tunes\n",
            "  Create fine-tune\n",
            "  List fine-tunes\n",
            "  Retrieve fine-tune\n",
            "  Cancel fine-tune\n",
            "  List fine-tune events\n",
            "  Delete fine-tune model\n",
            " Moderations\n",
            "  Create moderation\n",
            " Engines\n",
            "  List enginesDeprecated\n",
            "  Retrieve engineDeprecated\n",
            " Parameter details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text content of the first section (Introduction)\n",
        "print(api_reference.sub_sections[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKEH8StfXvp0",
        "outputId": "f042afa5-9528-4aae-fb8f-9a2a15dd715a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IntroductionYou can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library.To install the official Python bindings, run the following command:pip install openaiTo install the official Node.js library, run the following command in your Node.js project directory:npm install openai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten version of the sections\n",
        "[a.name for a in api_reference.flatten()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xZ7lVOwQlpq",
        "outputId": "8f695098-2964-4ea2-849a-1866648d35a9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OpenAI API reference',\n",
              " 'Introduction',\n",
              " 'Authentication',\n",
              " 'Making requests',\n",
              " 'Models',\n",
              " 'List models',\n",
              " 'Retrieve model',\n",
              " 'Completions',\n",
              " 'Create completion',\n",
              " 'Chat',\n",
              " 'Create chat completionBeta',\n",
              " 'Edits',\n",
              " 'Create edit',\n",
              " 'Images',\n",
              " 'Create imageBeta',\n",
              " 'Create image editBeta',\n",
              " 'Create image variationBeta',\n",
              " 'Embeddings',\n",
              " 'Create embeddings',\n",
              " 'Audio',\n",
              " 'Create transcriptionBeta',\n",
              " 'Create translationBeta',\n",
              " 'Files',\n",
              " 'List files',\n",
              " 'Upload file',\n",
              " 'Delete file',\n",
              " 'Retrieve file',\n",
              " 'Retrieve file content',\n",
              " 'Fine-tunes',\n",
              " 'Create fine-tune',\n",
              " 'List fine-tunes',\n",
              " 'Retrieve fine-tune',\n",
              " 'Cancel fine-tune',\n",
              " 'List fine-tune events',\n",
              " 'Delete fine-tune model',\n",
              " 'Moderations',\n",
              " 'Create moderation',\n",
              " 'Engines',\n",
              " 'List enginesDeprecated',\n",
              " 'Retrieve engineDeprecated',\n",
              " 'Parameter details']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "67myg8nKvaJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your own api key\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "openai.api_key = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCAm_xoXK8Oi",
        "outputId": "fbe160a9-7582-45a4-c15b-5555b5481f37"
      },
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question\n",
        "question = \"How can I get to known all the models available in the API?\""
      ],
      "metadata": {
        "id": "1zLec60QQ7Vd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"This is a list of sections containing the API reference of OpenAI: \\n{api_reference.get_index_text()}. \\nProvided the previous list of sections. Which section can better answer the following question: \\n\\\"{question}\\\"? \\nThe section:\""
      ],
      "metadata": {
        "id": "uvtMYQrvRGVz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Prompt to find which section answers the question\n"
      ],
      "metadata": {
        "id": "HLfOdV6meATy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omRWxqmtSQlI",
        "outputId": "62501a6a-a6ed-4358-a5e9-23d08a3cff6c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a list of sections containing the API reference of OpenAI: \n",
            "OpenAI API reference\n",
            " Introduction\n",
            " Authentication\n",
            " Making requests\n",
            " Models\n",
            "  List models\n",
            "  Retrieve model\n",
            " Completions\n",
            "  Create completion\n",
            " Chat\n",
            "  Create chat completionBeta\n",
            " Edits\n",
            "  Create edit\n",
            " Images\n",
            "  Create imageBeta\n",
            "  Create image editBeta\n",
            "  Create image variationBeta\n",
            " Embeddings\n",
            "  Create embeddings\n",
            " Audio\n",
            "  Create transcriptionBeta\n",
            "  Create translationBeta\n",
            " Files\n",
            "  List files\n",
            "  Upload file\n",
            "  Delete file\n",
            "  Retrieve file\n",
            "  Retrieve file content\n",
            " Fine-tunes\n",
            "  Create fine-tune\n",
            "  List fine-tunes\n",
            "  Retrieve fine-tune\n",
            "  Cancel fine-tune\n",
            "  List fine-tune events\n",
            "  Delete fine-tune model\n",
            " Moderations\n",
            "  Create moderation\n",
            " Engines\n",
            "  List enginesDeprecated\n",
            "  Retrieve engineDeprecated\n",
            " Parameter details. \n",
            "Provided the previous list of sections. Which section can better answer the following question: \n",
            "\"How can I get to known all the models available in the API?\"? \n",
            "The section:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open ended answer is good, but we need structure\n",
        "openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=0, max_tokens=20).choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MtVg5ZgXSjEI",
        "outputId": "413a1698-ea4e-4d4b-d312-57ca315dfd88"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \"Models\" \\nList models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the likelihood for all the sections (answer in the prompt)\n",
        "section_likelihoods = []\n",
        "for section in api_reference.flatten():\n",
        "  completion = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt+section.name, temperature=0, max_tokens=0, echo=True, logprobs=1)\n",
        "  section_likelihoods.append(sum(completion.choices[0].logprobs.token_logprobs[1:]))"
      ],
      "metadata": {
        "id": "Ab0aECNfzcV_"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the likelihoods (models and list models are the best)\n",
        "max_likelihood = section_likelihoods[0]\n",
        "max_section = api_reference.flatten()[0]\n",
        "for section, likelihood in zip(api_reference.flatten(), section_likelihoods):\n",
        "  print(section.name, likelihood)\n",
        "  if likelihood>max_likelihood:\n",
        "    max_section=section\n",
        "    max_likelihood=likelihood"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA3MscN9MaMC",
        "outputId": "17b2765b-c92c-4a06-ca3a-369c112423f4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API reference -546.3433321591601\n",
            "Introduction -546.3458362236801\n",
            "Authentication -548.6128641759998\n",
            "Making requests -544.6660877487195\n",
            "Models -532.7902207637338\n",
            "List models -528.42620743276\n",
            "Retrieve model -538.8091972026949\n",
            "Completions -545.5277878728415\n",
            "Create completion -549.9212066213499\n",
            "Chat -548.5513519629001\n",
            "Create chat completionBeta -552.1376227778402\n",
            "Edits -546.4442406511398\n",
            "Create edit -551.8922831572302\n",
            "Images -547.8282930549102\n",
            "Create imageBeta -553.9139928172501\n",
            "Create image editBeta -553.7375977768696\n",
            "Create image variationBeta -552.7827922148501\n",
            "Embeddings -549.14612271828\n",
            "Create embeddings -552.57049525262\n",
            "Audio -551.60806847418\n",
            "Create transcriptionBeta -553.1769849809197\n",
            "Create translationBeta -557.9149830902699\n",
            "Files -548.3277477801101\n",
            "List files -550.5578719745204\n",
            "Upload file -549.24956939049\n",
            "Delete file -549.2407333835198\n",
            "Retrieve file -553.8603904980902\n",
            "Retrieve file content -554.1357165980902\n",
            "Fine-tunes -551.0679240754097\n",
            "Create fine-tune -553.2274938205902\n",
            "List fine-tunes -554.8163459474802\n",
            "Retrieve fine-tune -556.8523778046\n",
            "Cancel fine-tune -556.87145576748\n",
            "List fine-tune events -564.5623875813196\n",
            "Delete fine-tune model -551.2498912036797\n",
            "Moderations -560.4887255993501\n",
            "Create moderation -552.5413510663101\n",
            "Engines -548.1869694244801\n",
            "List enginesDeprecated -549.3573256997003\n",
            "Retrieve engineDeprecated -550.7337444194402\n",
            "Parameter details -550.3117093046602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The section \\\"{max_section.name}\\\" has the maximum likelihood ({max_likelihood}) to answer the question: \\n{question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi3mCLmSVGIp",
        "outputId": "3f07364a-a938-4ca1-d708-12e2c8cfae2b"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The section \"List models\" has the maximum likelihood (-528.42620743276) to answer the question: \n",
            "How can I get to known all the models available in the API?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Include the content of the maximum likelihood section from the table of contents. Get the final answer"
      ],
      "metadata": {
        "id": "pV5jSkAxekGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_context = f\"This is a section from the OpenAI API reference that will be used to answer some questions:\\n\\n {max_section.text} \\n\\nTaking into account the previous information, {question}\"\n",
        "print(prompt_with_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWXrbwq4Vqee",
        "outputId": "0e6261d9-36ca-4db2-dd3c-c917aca9606f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a section from the OpenAI API reference that will be used to answer some questions:\n",
            "\n",
            " List modelsget https://api.openai.com/v1/modelsLists the currently available models, and provides basic information about each one such as the owner and availability.Example requestcurlSelect librarycurlpythonnode.jsCopy‍1\n",
            "2\n",
            "curl https://api.openai.com/v1/models \\\n",
            "  -H 'Authorization: Bearer YOUR_API_KEY'ResponseCopy‍1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": \"model-id-0\",\n",
            "      \"object\": \"model\",\n",
            "      \"owned_by\": \"organization-owner\",\n",
            "      \"permission\": [...]\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"model-id-1\",\n",
            "      \"object\": \"model\",\n",
            "      \"owned_by\": \"organization-owner\",\n",
            "      \"permission\": [...]\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"model-id-2\",\n",
            "      \"object\": \"model\",\n",
            "      \"owned_by\": \"openai\",\n",
            "      \"permission\": [...]\n",
            "    },\n",
            "  ],\n",
            "  \"object\": \"list\"\n",
            "} \n",
            "\n",
            "Taking into account the previous information, How can I get to known all the models available in the API?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5tYfigZRi6SE",
        "outputId": "ff4661cc-2985-412b-cc25-f5eddad9a6b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nTo get to know all the models available in the API, you can make a request to the endpoint https://api.openai.com/v1/models. This will return a list of all the models available, along with basic information about each one such as the owner and availability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "# Answer\n",
        "openai.Completion.create(model=\"text-davinci-003\", prompt=prompt_with_context, temperature=0, max_tokens=1000).choices[0].text"
      ]
    }
  ]
}